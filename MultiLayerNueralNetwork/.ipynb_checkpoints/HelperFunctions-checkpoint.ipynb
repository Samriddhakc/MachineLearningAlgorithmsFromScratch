{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement sigmoid activation function & Relu\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid at 0 is 0.5\n",
      "relu at 0 is 0\n",
      "relu at -ve is 0\n",
      "relu at +ve is [[0 0 1 2]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Test cases \n",
    "print(\"Sigmoid at 0 is \"+str(sigmoid(0)))\n",
    "print(\"relu at 0 is \"+str(relu(0)));\n",
    "print(\"relu at -ve is \"+str(relu(-1)));\n",
    "print(\"relu at +ve is \"+str(relu(np.array([[-1,0,1,2],[-1,-2,-3,-4]]))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement logit loss function\n",
    "def loss(m,A,Y):\n",
    "        logprobs = np.multiply(np.log(A),Y) + np.multiply(np.log(1 - A), 1 - Y)\n",
    "        cost = -1./m * np.sum(logprobs)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.544152856533524\n"
     ]
    }
   ],
   "source": [
    "#Test cases for the logit loss function\n",
    "A3=np.random.rand(3,)\n",
    "Y=np.array([0,1,2])\n",
    "m=Y.shape[0]\n",
    "print(loss(m,A3,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3=np.random.rand(3,1)\n",
    "Y=np.array([0,1,2])\n",
    "m=Y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement L2 Loss\n",
    "def L2_loss(m,A,Y):\n",
    "        J=(1./m)*np.sqrt(np.sum((np.subtract(A,Y))**2))\n",
    "        return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5773502691896257\n"
     ]
    }
   ],
   "source": [
    "A_test=np.random.rand(3,3)\n",
    "Y_test=np.array([0,1,2])\n",
    "Y_test=np.eye(3)[Y_test]\n",
    "m_test=Y_test.shape[0]\n",
    "print(L2_loss(m_test,np.eye(3),np.zeros((3,3))))\n",
    "#print(np.eye(3))\n",
    "#print(np.zeros((3,3)))\n",
    "#np.subtract(np.eye(3)-np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent algorithms \n",
    "\n",
    "#Implement gardient descnet with momentum. \n",
    "\n",
    "def gradient_descent_with_momentum(grad_params,params,exponential_weighted_grad_params,learning_rate=0.1,beta_1=0.9):\n",
    "    \n",
    "    W1=params[\"W1\"]\n",
    "    W2=params[\"W2\"]\n",
    "    b1=params[\"b1\"]\n",
    "    b2=params[\"b2\"]\n",
    "    \n",
    "    dW1_weighted=exponential_weighted_grad_params[\"dW1\"]\n",
    "    dW2_weighted=exponential_weighted_grad_params[\"dW2\"]\n",
    "    db1_weighted=exponential_weighted_grad_params[\"db1\"]\n",
    "    db2_weighted=exponential_weighted_grad_params[\"db2\"]\n",
    "    \n",
    "    \n",
    "    dW1=beta_1*dW1_weighted+(1-beta_1)*grad_params[\"dW1\"]\n",
    "    dW2=beta_1*dW2_weighted+(1-beta_1)*grad_params[\"dW2\"]\n",
    "    db1=beta_1*db1_weighted+(1-beta_1)*grad_params[\"db1\"]\n",
    "    db2=beta_1*db2_weighted+(1-beta_1)*grad_params[\"db2\"]\n",
    "\n",
    "    exponential_weighted_grad_params[\"dW1\"]=dW1\n",
    "    exponential_weighted_grad_params[\"dW2\"]=dW2\n",
    "    exponential_weighted_grad_params[\"db1\"]=db1\n",
    "    exponential_weighted_grad_params[\"db2\"]=db2\n",
    "    \n",
    "    W1=W1-learning_rate*dW1\n",
    "    W2=W2-learning_rate*dW2\n",
    "    b1=b1-learning_rate*db1\n",
    "    b2=b2-learning_rate*db2\n",
    "    \n",
    "    return W1,W2,b1,b2,exponential_weighted_grad_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_normal(grad_params,params,learning_rate=0.1):          \n",
    "    \n",
    "    W1=params[\"W1\"]\n",
    "    W2=params[\"W2\"]\n",
    "    b1=params[\"b1\"]\n",
    "    b2=params[\"b2\"]\n",
    "    \n",
    "    W1=W1-learning_rate*grad_params[\"dW1\"]\n",
    "    W2=W2-learning_rate*grad_params[\"dW2\"]\n",
    "    b1=b1-learning_rate*grad_params[\"db1\"]\n",
    "    b2=b2-learning_rate*grad_params[\"db2\"]\n",
    "    \n",
    "    return W1,W2,b1,b2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(grad_params,params,rms_prop_params,learning_rate=0.1,beta_2=0.99,epsilon=1e-7):\n",
    "    \n",
    "    W1=params[\"W1\"]\n",
    "    W2=params[\"W2\"]\n",
    "    b1=params[\"b1\"]\n",
    "    b2=params[\"b2\"]\n",
    "    \n",
    "    dSW1=rms_prop_params[\"dSW1\"]\n",
    "    dSW2=rms_prop_params[\"dSW2\"]\n",
    "    dSb1=rms_prop_params[\"dSb1\"]\n",
    "    dSb2=rms_prop_params[\"dSb2\"]\n",
    "    \n",
    "    \n",
    "    dSW1=beta_2*dSW1+(1-beta_2)*(((grad_params[\"dW1\"])**2)+epsilon)\n",
    "    dSW2=beta_2*dSW2+(1-beta_2)*(((grad_params[\"dW2\"])**2)+epsilon)\n",
    "    dSb1=beta_2*dSb1+(1-beta_2)*(((grad_params[\"db1\"])**2)+epsilon)\n",
    "    dSb2=beta_2*dSb2+(1-beta_2)*(((grad_params[\"db2\"])**2)+epsilon)\n",
    "\n",
    "    rms_prop_params[\"dSW1\"]=dSW1\n",
    "    rms_prop_params[\"dSW2\"]=dSW2\n",
    "    rms_prop_params[\"dSb1\"]=dSb1\n",
    "    rms_prop_params[\"dSb2\"]=dSb2\n",
    "    \n",
    "    W1=W1-learning_rate*np.divide(grad_params[\"dW1\"],np.sqrt(dSW1))\n",
    "    W2=W2-learning_rate*np.divide(grad_params[\"dW2\"],np.sqrt(dSW2))\n",
    "    b1=b1-learning_rate*np.divide(grad_params[\"db1\"],np.sqrt(dSb1))\n",
    "    b2=b2-learning_rate*np.divide(grad_params[\"db2\"],np.sqrt(dSb2))\n",
    "    \n",
    "    return W1,W2,b1,b2,rms_prop_params\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(grad_params,params,rms_prop_params,exponential_weighted_grad_params,learning_rate=0.1,beta_1=0.9,beta_2=0.99,epsilon=1e-7):\n",
    "    \n",
    "    W1=params[\"W1\"]\n",
    "    W2=params[\"W2\"]\n",
    "    b1=params[\"b1\"]\n",
    "    b2=params[\"b2\"]\n",
    "    \n",
    "    dSW1=rms_prop_params[\"dSW1\"]\n",
    "    dSW2=rms_prop_params[\"dSW2\"]\n",
    "    dSb1=rms_prop_params[\"dSb1\"]\n",
    "    dSb2=rms_prop_params[\"dSb2\"]\n",
    "    \n",
    "    \n",
    "    dSW1=beta_2*dSW1+(1-beta_2)*(((grad_params[\"dW1\"])**2)+epsilon)\n",
    "    dSW2=beta_2*dSW2+(1-beta_2)*(((grad_params[\"dW2\"])**2)+epsilon)\n",
    "    dSb1=beta_2*dSb1+(1-beta_2)*(((grad_params[\"db1\"])**2)+epsilon)\n",
    "    dSb2=beta_2*dSb2+(1-beta_2)*(((grad_params[\"db2\"])**2)+epsilon)\n",
    "\n",
    "    rms_prop_params[\"dSW1\"]=dSW1\n",
    "    rms_prop_params[\"dSW2\"]=dSW2\n",
    "    rms_prop_params[\"dSb1\"]=dSb1\n",
    "    rms_prop_params[\"dSb2\"]=dSb2\n",
    "    \n",
    "    dW1_weighted=exponential_weighted_grad_params[\"dW1\"]\n",
    "    dW2_weighted=exponential_weighted_grad_params[\"dW2\"]\n",
    "    db1_weighted=exponential_weighted_grad_params[\"db1\"]\n",
    "    db2_weighted=exponential_weighted_grad_params[\"db2\"]\n",
    "    \n",
    "    \n",
    "    dW1=beta_1*dW1_weighted+(1-beta_1)*grad_params[\"dW1\"]\n",
    "    dW2=beta_1*dW2_weighted+(1-beta_1)*grad_params[\"dW2\"]\n",
    "    db1=beta_1*db1_weighted+(1-beta_1)*grad_params[\"db1\"]\n",
    "    db2=beta_1*db2_weighted+(1-beta_1)*grad_params[\"db2\"]\n",
    "\n",
    "    exponential_weighted_grad_params[\"dW1\"]=dW1\n",
    "    exponential_weighted_grad_params[\"dW2\"]=dW2\n",
    "    exponential_weighted_grad_params[\"db1\"]=db1\n",
    "    exponential_weighted_grad_params[\"db2\"]=db2\n",
    "    \n",
    "    \n",
    "    W1=W1-learning_rate*np.divide(dW1,np.sqrt(dSW1))\n",
    "    W2=W2-learning_rate*np.divide(dW2,np.sqrt(dSW2))\n",
    "    b1=b1-learning_rate*np.divide(db1,np.sqrt(dSb1))\n",
    "    b2=b2-learning_rate*np.divide(db2,np.sqrt(dSb2))\n",
    "    \n",
    "    return W1,W2,b1,b2,rms_prop_params,exponential_weighted_grad_params\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
